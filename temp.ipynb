{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimage\n",
    "import pywt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import ImageToImage2D, JointTransform2D\n",
    "# parent_dir = '/mnt/d/Documents/Research/disWT/T1fusion_out/'\n",
    "# for k in range(3):\n",
    "#     img_folder = '{}'.format(k) + '_out/'\n",
    "#     for i in range(2):\n",
    "        \n",
    "#         sub_directory = '{}_{}'.format(k, i) + '_out/'\n",
    "#         for j in range(2):\n",
    "#             _file = '{}_{}_{}'.format(k, i, j)\n",
    "#             path = os.path.join(parent_dir, img_folder, sub_directory, _file)\n",
    "#             os.makedirs(path)\n",
    "tf_train = JointTransform2D(crop=None, p_flip=0.5, color_jitter_params=None, long_mask=True)\n",
    "train_dataset = ImageToImage2D('/mnt/d/Documents/Research/Train Folder test/', tf_train)\n",
    "\n",
    "dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "# img_temp = mpimage.imread('T1fusion/1/IMG-0004-00002IMG-0004-00001.png')\n",
    "# imgs = [mpimage.imread(file) for file in ('T1fusion/1/IMG-0004-00002IMG-0004-00001.png', 'T1fusion/1/IMG-0004-00004IMG-0004-00003.png', 'T1fusion/1/IMG-0004-00004IMG-0004-00003.png')]\n",
    "\"\"\"\n",
    " converting the list of ndarrays to an ndarray before converting to tensor.\n",
    " Reason: the process of converting to a tensor from an ndarray is extremely slow.\n",
    "\"\"\"\n",
    "# np_imgs = np.array(imgs)\n",
    "my_dpi = 144\n",
    "count_batch = 0\n",
    "for batch_idx, (X_batch, y_batch, *rest) in enumerate(dataloader):\n",
    "    \n",
    "    # x_batch_np = X_batch.detach().cpu().numpy()\n",
    "    LL_list, LH_list, HL_list, HH_list = [], [], [], []\n",
    "    # for _ in range(len(x_batch_np)): # len(x_batch_np) finds out the number of images in the dataset\n",
    "    count = 0\n",
    "        # imgs = [mpimage.imread(file) for file in X_batch] #Reading all the images in a particular subdirectory, say, 1 in T1fusion\n",
    "        # tensor_imgs = torch.tensor(imgs)\n",
    "    for img in X_batch:\n",
    "        count += 1\n",
    "\n",
    "        for _ in img:\n",
    "            coeffs2 = pywt.dwt2(_, 'db3')\n",
    "            LL, (LH, HL, HH) = coeffs2\n",
    "            \n",
    "            LL_list.append(LL)\n",
    "            LH_list.append(LH)\n",
    "            HL_list.append(HL)\n",
    "            HH_list.append(HH)\n",
    "    # print(count)\n",
    "print(batch_idx)\n",
    "# for i, a in enumerate([LL, LH, HL, HH]):\n",
    "    \n",
    "#     plt.figure(figsize=(256/my_dpi, 256/my_dpi), dpi=my_dpi)\n",
    "\n",
    "#     plt.imshow(a, interpolation='nearest', cmap=plt.cm.gray)\n",
    "#     plt.axis('off')\n",
    "#     plt.tight_layout(pad=0) \n",
    "#     plt.savefig('plot.png')\n",
    "    \n",
    "# #     plt.show()\n",
    "#     print(count)\n",
    "#     LL_list = np.array(LL_list)\n",
    "#     LH_list = np.array(LH_list)\n",
    "#     HL_list = np.array(HL_list)\n",
    "#     HH_list = np.array(HH_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ten_imgs = torch.tensor(np_imgs)\n",
    "# print(ten_img)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# coeff2 = pywt.dwt2(np_imgs[0], 'db3', mode='periodization')\n",
    "# LL, (LH, HL, HH) = coeff2\n",
    "\n",
    "# print(type(LL))\n",
    "# print(type(LH))\n",
    "# print(type(HL))\n",
    "# print(type(HH))\n",
    "\n",
    "# for i, a in enumerate([LL, LH, HL, HH]):\n",
    "#     plt.figure(figsize=(1,1), dpi=256)\n",
    "#     plt.imshow(a, interpolation='nearest', cmap=plt.cm.gray)\n",
    "#     plt.axis('off')\n",
    "#     plt.tight_layout(pad=0)\n",
    "#     plt.savefig('plot.png')\n",
    "    \n",
    "#     plt.show()\n",
    "\n",
    "# idwt_fig = pywt.idwt2(coeff2, 'db3')\n",
    "# print(type(idwt_fig))\n",
    "# plt.figure(figsize=(1,1), dpi=256)\n",
    "# plt.imshow(idwt_fig, interpolation='nearest', cmap=plt.cm.gray)\n",
    "# plt.axis('off')\n",
    "# plt.tight_layout(pad=0)\n",
    "# plt.savefig('idwt_out.png')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "47f5d56a57c9fe1a864c9088028f92f0c4664d193fd5ae590a8d2ad9c8f2a6b4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('Research': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
